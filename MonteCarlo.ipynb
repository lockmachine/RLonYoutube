{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.モンテカルロ法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 「ゲームをやってみた結果、もらった報酬」を元に価値計算\n",
    "* 今回の「価値」は、Q(s, a)で表される、「行動の価値」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.Q = {}\n",
    "        self.initialize()\n",
    "    \n",
    "    # Q値（各状態における各行動の価値）を初期化\n",
    "    def initialize(self):\n",
    "        for s in range(self.env.nS):\n",
    "            self.Q[s] = [0] * self.env.nA\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 0, 0, 0],\n",
       " 1: [0, 0, 0, 0],\n",
       " 2: [0, 0, 0, 0],\n",
       " 3: [0, 0, 0, 0],\n",
       " 4: [0, 0, 0, 0],\n",
       " 5: [0, 0, 0, 0],\n",
       " 6: [0, 0, 0, 0],\n",
       " 7: [0, 0, 0, 0],\n",
       " 8: [0, 0, 0, 0],\n",
       " 9: [0, 0, 0, 0],\n",
       " 10: [0, 0, 0, 0],\n",
       " 11: [0, 0, 0, 0],\n",
       " 12: [0, 0, 0, 0],\n",
       " 13: [0, 0, 0, 0],\n",
       " 14: [0, 0, 0, 0],\n",
       " 15: [0, 0, 0, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc = MonteCarlo(env)\n",
    "mc.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1モンテカルロ法の理論\n",
    "Q(s,a)は、「状態sでaという行動をしたときに、ゲーム終了までに得られる報酬」で決める。\n",
    "\n",
    "$$G_t = r_{t+1} + r_{t+2} + \\cdots + r_T$$\n",
    "\n",
    "今回はゲーム終了時にしか報酬をもらえないため、結局は$G_t = r_T$となる\n",
    "\n",
    "ただ、未来でもらえる報酬は不確かな値なので、「割引」$\\gamma$をかける。\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots + \\gamma^{T-t-1} r_T$$\n",
    "これを、「割引現在価値」という。\n",
    "$$Q(s,a)  Q(s,a) + \\alpha (G_t - Q(s,a))$$\n",
    "\n",
    "$\\alpha$は、「学習率」で0から1を取る。Qの更新は1ゲーム終了したら行われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.Q = {}\n",
    "        self.initialize()\n",
    "    \n",
    "    # Q値（各状態における各行動の価値）を初期化\n",
    "    def initialize(self):\n",
    "        for s in range(self.env.nS):\n",
    "            self.Q[s] = [0] * self.env.nA\n",
    "        self.env.reset()\n",
    "    \n",
    "    def learn(self, episode_count=10, gamma=0.9):\n",
    "        for e in range(episode_count):\n",
    "            done = False\n",
    "            experience_log = []\n",
    "            s = 0\n",
    "            self.env.reset()\n",
    "            \n",
    "            # エピソードを実行\n",
    "            while not done:\n",
    "                # とりあえずランダムに行動を選択\n",
    "                a = np.random.randint(self.env.nA)\n",
    "                # 行動を実行して、次の状態:next_s、報酬:reward、終了したかどうか:doneを取得する\n",
    "                next_s, reward, done, _ = self.env.step(a)\n",
    "                #　「現在の状態」から「ある行動」をとったときの「価値」をログとして保存する\n",
    "                experience_log.append({'state':s, 'action':a, 'reward':reward})\n",
    "                s = next_s\n",
    "            \n",
    "            # エピソードで取得したログからQ値を更新\n",
    "            for t, experience in enumerate(experience_log):\n",
    "                G = 0\n",
    "                i = 0\n",
    "                for j in range(t, len(experience_log)):\n",
    "                    G += (gamma**i) * experience_log[j]['reward']\n",
    "                    i += 1\n",
    "                s = experience['state']\n",
    "                a = experience['action']\n",
    "                self.Q[s][a] += alpha * (G)\n",
    "        return experience_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 0, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 0, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 0, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 0, 'reward': 0.0},\n",
       " {'state': 0, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 1, 'reward': 0.0},\n",
       " {'state': 4, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 3, 'reward': 0.0},\n",
       " {'state': 0, 'action': 2, 'reward': 0.0},\n",
       " {'state': 1, 'action': 1, 'reward': 0.0}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc = MonteCarlo(env)\n",
    "mc.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.0, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.0, True, {'prob': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 0.0, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1.0, True, {'prob': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 0, True, {'prob': 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 0, True, {'prob': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
